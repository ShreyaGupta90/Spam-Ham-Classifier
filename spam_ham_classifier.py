# -*- coding: utf-8 -*-
"""Spam Ham Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qqhTWuFRNZDjkXuIy4WYnQil0MZpcSTZ
"""

# importing the Dataset
#ham = not spam

import pandas as pd
messages = pd.read_csv('/content/SMSSpamCollection.txt', sep='\t',
                           names=["label", "message"])

messages

messages.shape

messages['message'].loc[100]

messages['message'].loc[451]

#Data cleaning and preprocessing
import re
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

corpus = []
for i in range(0, len(messages)):
    review = re.sub('[^a-zA-Z0-9]', ' ', messages['message'][i])
    review = review.lower()
    review = review.split()

    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

corpus

# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=2500,binary=True,ngram_range=(2,2)) #it takes top maximum occuring 2500 features
X = cv.fit_transform(corpus).toarray()

X

X[1]

X.shape

y=pd.get_dummies(messages['label'])
y=y.iloc[:,1].values

y

# Train Test Split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

X_train, y_train

from sklearn.naive_bayes import MultinomialNB
spam_detect_model = MultinomialNB().fit(X_train, y_train)

#prediction
y_pred=spam_detect_model.predict(X_test)

from sklearn.metrics import accuracy_score,classification_report

score=accuracy_score(y_test,y_pred)
print(score)

from sklearn.metrics import classification_report
print(classification_report(y_pred,y_test))

# Creating the TFIDF model
from sklearn.feature_extraction.text import TfidfVectorizer
tv = TfidfVectorizer(max_features=2500,ngram_range=(1,2)) # one feature to the combination of bi feature
X = tv.fit_transform(corpus).toarray()

# Train Test Split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

from sklearn.naive_bayes import MultinomialNB
spam_detect_model = MultinomialNB().fit(X_train, y_train)

#prediction
y_pred=spam_detect_model.predict(X_test)

score=accuracy_score(y_test,y_pred)
print(score)

from sklearn.metrics import classification_report
print(classification_report(y_pred,y_test))

from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier()
classifier.fit(X_train, y_train)

y_pred=classifier.predict(X_test)

print(accuracy_score(y_test,y_pred))
print(classification_report(y_pred,y_test))

"""
## Word2vec Implementation"""

!pip install gensim

import gensim.downloader as api

wv = api.load('word2vec-google-news-300')

vec_king = wv['king']

vec_king # having 300 dimensions

from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()

import nltk
nltk.download('wordnet')
nltk.download('punkt_tab')
corpus = []
for i in range(0, len(messages)):
    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])
    review = review.lower()
    review = review.split()

    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

corpus

from nltk import sent_tokenize
from gensim.utils import simple_preprocess

corpus[0]

nltk.download('punkt')
nltk.download('punkt_tab')
words = []
for message_text in corpus:
    tokenized_sentences = sent_tokenize(message_text)
    all_words_for_message = []
    for sentence_str in tokenized_sentences:
        all_words_for_message.extend(simple_preprocess(sentence_str))
    words.append(all_words_for_message)

words

import gensim

### Lets train Word2vec from scratch
model=gensim.models.Word2Vec(words,window=5,min_count=2)

model.wv.index_to_key

model.corpus_count

model.epochs

model.wv.similar_by_word('kid')

model.wv['kid'].shape

import numpy as np
# def avg_word2vec(doc):
#     return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)

def avg_word2vec(doc):                                                  # it is using cbow here; 100 features is created by cbow
    vectors = [model.wv[word] for word in doc if word in model.wv]

    if len(vectors) == 0:
        return np.zeros(model.vector_size)   # ‚Üê FIXED (100 dims)

    return np.mean(vectors, axis=0)

!pip install tqdm

from tqdm import tqdm

words[73]

type(model.wv.index_to_key)

words

#apply for the entire sentences
X=[]
for i in tqdm(range(len(words))):
    # print("Hello",i)
    X.append(avg_word2vec(words[i]))

type(X)

bad = [i for i, x in enumerate(X) if not hasattr(x, "__len__") or len(x) != model.vector_size]  # this indices value is not having 100 dimensions so we have to fix it
bad[:10], len(bad)

X = [
    x if hasattr(x, "__len__") and len(x) == model.vector_size
    else np.zeros(model.vector_size)
    for x in X
]
set(len(x) for x in X)

# X_new=np.array(X)
X_new = np.vstack(X)

X_new    #all input features

words[0]   # words at 0th index is:

X_new[0]    # it is one sentence input featue; for words at 0th index its total dimensions vectors is 100:

X_new.shape

y

len(X_new), len(y)

y = y[:len(X_new)]
len(X_new), len(y)

# Train Test Split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.20, random_state = 0)

from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier()
classifier.fit(X_train, y_train)
y_pred=classifier.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(classification_report(y_pred,y_test))

